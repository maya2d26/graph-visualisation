{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset\n",
    "from dgl.nn.pytorch import GraphConv\n",
    "\n",
    "import networkx as nx\n",
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data.generate as gen\n",
    "import data.visualise as vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs = gen.read_all_graphs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "9312\n"
     ]
    }
   ],
   "source": [
    "# information abaout number of nodes\n",
    "min_nodes = 10000\n",
    "max_nodes = 0\n",
    "for g in graphs:\n",
    "    if g.number_of_nodes() < min_nodes:\n",
    "        min_nodes = g.number_of_nodes()\n",
    "    if g.number_of_nodes() > max_nodes:\n",
    "        max_nodes = g.number_of_nodes()\n",
    "\n",
    "print(min_nodes)\n",
    "print(max_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create position matrix for all nodes\n",
    "all_positions = []\n",
    "for g in graphs:\n",
    "    pos_list = []\n",
    "    for node in g.nodes:\n",
    "        pos_list.append(g.nodes[node]['pos'])\n",
    "        g.nodes[node]['pos'] = torch.tensor(g.nodes[node]['pos']).clone().detach()\n",
    "    all_positions.append(torch.tensor(np.array(pos_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 174, 'type': 'random', 'algorithm': 'spring'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = graphs[134]\n",
    "g.graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scale(positions):\n",
    "    min_val = np.min(positions)\n",
    "    max_val = np.max(positions)\n",
    "    scaled_positions = (positions - min_val) / (max_val - min_val)\n",
    "    return scaled_positions, min_val, max_val\n",
    "\n",
    "def reverse_min_max_scale(scaled_positions, min_val, max_val):\n",
    "    positions = scaled_positions * (max_val - min_val) + min_val\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, graphs):\n",
    "        self.graphs = []\n",
    "        self.positions = []\n",
    "        self.min_max_for_rescaling = []\n",
    "        self.original_positions = []\n",
    "        for g in graphs:\n",
    "            features = []\n",
    "            for node in g.nodes:\n",
    "                features.append(g.nodes[node]['pos'].detach().numpy())\n",
    "\n",
    "            #scale the position data to [0, 1]\n",
    "            scaled_features, min_value, max_value = min_max_scale(np.array(features))\n",
    "            \n",
    "            scaled_features = torch.tensor(scaled_features)\n",
    "            dgl_g = dgl.from_networkx(g)\n",
    "            min_max_values = (min_value,max_value)\n",
    "            #dgl_g.ndata['pos'] = features\n",
    "\n",
    "            self.graphs.append(dgl_g)\n",
    "            self.positions.append(scaled_features.float())\n",
    "            self.original_positions.append(torch.tensor(np.array(features)).float())\n",
    "            self.min_max_for_rescaling.append(min_max_values)\n",
    "\n",
    "            # sanity check\n",
    "            reversed = reverse_min_max_scale(scaled_features.numpy(), min_value, max_value)\n",
    "            try:\n",
    "                assert np.array_equal(np.round(reversed, decimals=4), np.round(np.array(features), decimals=4))\n",
    "            except AssertionError as e:\n",
    "                errors = [(x,y) for x,y in zip(np.round(reversed,decimals=4), np.round(np.array(features), decimals=4)) if not np.array_equal(x,y)]\n",
    "                for x, y in errors:\n",
    "                    print(f\"Inaccurate scaling on graph {g.graph['id']}: rescale: {x}, original: {y}\")               \n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a sample and its corresponding label based on the given index\n",
    "        return self.graphs[idx], self.positions[idx], self.original_positions[idx], self.min_max_for_rescaling[idx]\n",
    "\n",
    "def custom_collate(batch):\n",
    "    graphs, features, unscaled_features, min_max = zip(*batch)\n",
    "    batched_graphs = dgl.batch(graphs)\n",
    "    batched_features = torch.cat(features, dim=0)\n",
    "    batched_unscaled_features = torch.cat(unscaled_features, dim=0)\n",
    "    return batched_graphs, batched_features, batched_unscaled_features, min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(indices):\n",
    "    train_indices, val_indices_= train_test_split(indices, test_size=0.3)\n",
    "    val_indices, test_indices = train_test_split(val_indices_, test_size=0.5)\n",
    "\n",
    "    return train_indices, val_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "random_graphs = [g for g in graphs if g.graph['type'] == 'random']\n",
    "star_graphs = [g for g in graphs if g.graph['type'] == 'star']\n",
    "grid_graphs = [g for g in graphs if g.graph['type'] == 'grid']\n",
    "\n",
    "train_graphs = []\n",
    "valid_graphs = []\n",
    "test_graphs = []\n",
    "\n",
    "# random graphs\n",
    "train_indices, val_indices, test_indices = train_val_test_split(np.arange(len(random_graphs)))\n",
    "\n",
    "train_graphs_ = [random_graphs[i] for i in train_indices]\n",
    "valid_graphs_ = [random_graphs[i] for i in val_indices]\n",
    "test_graphs_ = [random_graphs[i] for i in test_indices]\n",
    "\n",
    "train_graphs = train_graphs + train_graphs_\n",
    "valid_graphs = valid_graphs + valid_graphs_\n",
    "test_graphs = test_graphs + test_graphs_\n",
    "\n",
    "# star graphs\n",
    "train_indices, val_indices, test_indices = train_val_test_split(np.arange(len(star_graphs)))\n",
    "\n",
    "train_graphs_ = [star_graphs[i] for i in train_indices]\n",
    "valid_graphs_ = [star_graphs[i] for i in val_indices]\n",
    "test_graphs_ = [star_graphs[i] for i in test_indices]\n",
    "\n",
    "train_graphs = train_graphs + train_graphs_\n",
    "valid_graphs = valid_graphs + valid_graphs_\n",
    "test_graphs = test_graphs + test_graphs_\n",
    "\n",
    "# grid graphs\n",
    "train_indices, val_indices, test_indices = train_val_test_split(np.arange(len(grid_graphs)))\n",
    "\n",
    "train_graphs_ = [grid_graphs[i] for i in train_indices]\n",
    "valid_graphs_ = [grid_graphs[i] for i in val_indices]\n",
    "test_graphs_ = [grid_graphs[i] for i in test_indices]\n",
    "\n",
    "train_graphs = train_graphs + train_graphs_\n",
    "valid_graphs = valid_graphs + valid_graphs_\n",
    "test_graphs = test_graphs + test_graphs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inaccurate scaling on graph 283: rescale: [0.599  0.5676], original: [0.599  0.5675]\n",
      "Inaccurate scaling on graph 283: rescale: [0.0995 0.4944], original: [0.0995 0.4945]\n",
      "Inaccurate scaling on graph 283: rescale: [5.09224e+01 4.94000e-02], original: [5.09224e+01 4.93000e-02]\n",
      "Inaccurate scaling on graph 283: rescale: [0.3632 0.95  ], original: [0.3632 0.9501]\n",
      "Inaccurate scaling on graph 283: rescale: [0.2985 0.2148], original: [0.2984 0.2148]\n",
      "Inaccurate scaling on graph 913: rescale: [0.3776 0.9434], original: [0.3776 0.9435]\n"
     ]
    }
   ],
   "source": [
    "# create dataloaders\n",
    "batch_size = 32\n",
    "train_ds = GraphDataset(train_graphs)\n",
    "train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle = True, collate_fn=custom_collate, pin_memory=True)\n",
    "val_ds = GraphDataset(valid_graphs)\n",
    "val_dl = DataLoader(val_ds, batch_size = batch_size, collate_fn=custom_collate, pin_memory=True)\n",
    "test_ds = GraphDataset(test_graphs)\n",
    "test_dl = DataLoader(test_ds, batch_size = batch_size, collate_fn=custom_collate, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "  def __init__(self, in_feats, n_hidden, n_classes, n_layers, activation = F.relu, dropout = 0):\n",
    "    super(GCN, self).__init__()\n",
    "    self.layers = nn.ModuleList()\n",
    "    # input layer\n",
    "    self.layers.append(GraphConv(in_feats, n_hidden, activation=activation))\n",
    "    # hidden layers\n",
    "    for i in range(n_layers - 1):\n",
    "      self.layers.append(GraphConv(n_hidden, n_hidden, activation=activation))\n",
    "    # output layer\n",
    "    self.layers.append(GraphConv(n_hidden, n_classes))\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "  def forward(self, g, features):\n",
    "    # the H matrix is the feature matrix\n",
    "    h = features\n",
    "    # the matrix passes through each layer\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      if i != 0:\n",
    "        h = self.dropout(h)       #dropout\n",
    "      h = layer(g, h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (layers): ModuleList(\n",
       "    (0): GraphConv(in=2, out=48, normalization=both, activation=<function relu at 0x00000171654C96C0>)\n",
       "    (1-4): 4 x GraphConv(in=48, out=48, normalization=both, activation=<function relu at 0x00000171654C96C0>)\n",
       "    (5): GraphConv(in=48, out=2, normalization=both, activation=None)\n",
       "  )\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialising the model\n",
    "model = GCN(in_feats = 2, n_hidden = 48, n_classes = 2, n_layers = 5, dropout = 0)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([448, 2])\n",
      "torch.Size([4745, 2])\n",
      "torch.Size([1392, 2])\n",
      "torch.Size([4074, 2])\n",
      "torch.Size([768, 2])\n",
      "torch.Size([392, 2])\n",
      "torch.Size([2304, 2])\n",
      "torch.Size([3496, 2])\n",
      "torch.Size([1296, 2])\n",
      "torch.Size([2002, 2])\n"
     ]
    }
   ],
   "source": [
    "for g in graphs[:10]:\n",
    "    features = []\n",
    "    for node in g.nodes:\n",
    "        features.append(g.nodes[node]['pos'].detach().numpy())\n",
    "    \n",
    "    h = torch.tensor(np.array(features))\n",
    "    dgl_g = dgl.from_networkx(g)\n",
    "\n",
    "    h = model(dgl_g, h)\n",
    "    print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for early stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=1):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > self.min_validation_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a train function\n",
    "def train(model, num_epochs, optimizer, loss, train_dl, valid_dl, patience = -1, verbose = True):\n",
    "    all_logits = []\n",
    "    all_train_loss = []\n",
    "    all_val_loss = []\n",
    "\n",
    "    if(patience != -1):\n",
    "        early_stopper = EarlyStopping(patience)\n",
    "\n",
    "    # if available using cuda\n",
    "    #model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_losses = []\n",
    "        for batch in train_dl:\n",
    "            batched_graph, features,_, _ = batch\t\t\t\n",
    "            logits = model(dgl.add_self_loop(batched_graph), torch.tensor(np.zeros_like(features)).float())\n",
    "            assert features.shape == logits.shape\n",
    "            #compute loss\n",
    "            train_loss = loss(logits, features.float())\n",
    "                \n",
    "            #training the model\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            all_logits.append(logits.detach())\n",
    "            train_losses.append(train_loss.item())\t\n",
    "        train_losses = np.array(train_losses)\n",
    "        all_train_loss.append(train_losses)\n",
    "        \n",
    "        # check validation\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch in valid_dl:\n",
    "                batched_graph, features, _, _ = batch\n",
    "                logits = model(dgl.add_self_loop(batched_graph), torch.tensor(np.zeros_like(features)).float())\n",
    "                val_loss = loss(logits, features.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "        val_losses = np.array(val_losses)\n",
    "        all_val_loss.append(val_losses)\n",
    "        \n",
    "        #early stopping\n",
    "        if patience != -1 and early_stopper.early_stop(val_loss):\n",
    "            if verbose:\n",
    "                print(\"Early stopping has occured!\")\n",
    "            break\n",
    "            \n",
    "        if verbose:\n",
    "            print('Epoch %d | Avg train Loss: %.4f | Min train Loss: %.4f | Max train Loss: %.4f | Avg valid Loss: %.4f | Min valid Loss: %.4f | Max valid Loss: %.4f' % \n",
    "                        (epoch, np.mean(train_losses), np.min(train_losses), np.max(train_losses), np.mean(val_losses), np.min(val_losses), np.max(val_losses)))\n",
    "                        \n",
    "    return all_logits, np.array(all_train_loss), np.array(all_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Avg train Loss: 0.0676 | Min train Loss: 0.0507 | Max train Loss: 0.0932 | Avg valid Loss: 0.0581 | Min valid Loss: 0.0511 | Max valid Loss: 0.0815\n",
      "Epoch 1 | Avg train Loss: 0.0664 | Min train Loss: 0.0528 | Max train Loss: 0.0823 | Avg valid Loss: 0.0611 | Min valid Loss: 0.0554 | Max valid Loss: 0.0756\n",
      "Epoch 2 | Avg train Loss: 0.0687 | Min train Loss: 0.0507 | Max train Loss: 0.0902 | Avg valid Loss: 0.0583 | Min valid Loss: 0.0501 | Max valid Loss: 0.0867\n",
      "Epoch 3 | Avg train Loss: 0.0682 | Min train Loss: 0.0524 | Max train Loss: 0.0927 | Avg valid Loss: 0.0579 | Min valid Loss: 0.0504 | Max valid Loss: 0.0837\n",
      "Epoch 4 | Avg train Loss: 0.0665 | Min train Loss: 0.0545 | Max train Loss: 0.0987 | Avg valid Loss: 0.0602 | Min valid Loss: 0.0550 | Max valid Loss: 0.0756\n",
      "Epoch 5 | Avg train Loss: 0.0670 | Min train Loss: 0.0540 | Max train Loss: 0.0863 | Avg valid Loss: 0.0567 | Min valid Loss: 0.0483 | Max valid Loss: 0.0840\n",
      "Epoch 6 | Avg train Loss: 0.0662 | Min train Loss: 0.0507 | Max train Loss: 0.0810 | Avg valid Loss: 0.0631 | Min valid Loss: 0.0553 | Max valid Loss: 0.0796\n",
      "Epoch 7 | Avg train Loss: 0.0669 | Min train Loss: 0.0518 | Max train Loss: 0.0892 | Avg valid Loss: 0.0572 | Min valid Loss: 0.0507 | Max valid Loss: 0.0781\n",
      "Epoch 8 | Avg train Loss: 0.0664 | Min train Loss: 0.0540 | Max train Loss: 0.0797 | Avg valid Loss: 0.0576 | Min valid Loss: 0.0513 | Max valid Loss: 0.0792\n",
      "Epoch 9 | Avg train Loss: 0.0679 | Min train Loss: 0.0539 | Max train Loss: 0.0868 | Avg valid Loss: 0.0612 | Min valid Loss: 0.0527 | Max valid Loss: 0.0892\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "loss = nn.MSELoss()\n",
    "all_logits, all_train_loss, all_val_loss = train(model, 10, optimizer, loss, train_dl, val_dl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06378424, 0.06338058, 0.05065175, 0.08257305, 0.06733917,\n",
       "        0.0587659 , 0.05929109, 0.06851999, 0.0736275 , 0.07758753,\n",
       "        0.06939198, 0.05800937, 0.08596684, 0.06039057, 0.07102326,\n",
       "        0.06540596, 0.06403052, 0.05954872, 0.05647984, 0.06970081,\n",
       "        0.09317273, 0.06949528],\n",
       "       [0.05825505, 0.05430376, 0.07544778, 0.06165404, 0.07528561,\n",
       "        0.07537641, 0.05833545, 0.05889522, 0.07133072, 0.05929767,\n",
       "        0.05488056, 0.07834159, 0.07414039, 0.05281418, 0.05599844,\n",
       "        0.0823352 , 0.06708096, 0.06925076, 0.06405424, 0.07715543,\n",
       "        0.06232596, 0.07486474],\n",
       "       [0.05899919, 0.06565566, 0.05820832, 0.05800349, 0.07515676,\n",
       "        0.05500498, 0.0823854 , 0.09023158, 0.08900127, 0.05719982,\n",
       "        0.06351675, 0.07476663, 0.07236217, 0.06467618, 0.07024312,\n",
       "        0.06443466, 0.06174158, 0.075865  , 0.08478507, 0.0506903 ,\n",
       "        0.08024418, 0.05845245],\n",
       "       [0.08546553, 0.06421471, 0.05556084, 0.05637836, 0.05399564,\n",
       "        0.0707759 , 0.08227175, 0.06860631, 0.08212285, 0.07606436,\n",
       "        0.07428276, 0.07059408, 0.05488832, 0.06975576, 0.06382946,\n",
       "        0.05760631, 0.06013598, 0.05717469, 0.09267069, 0.07795089,\n",
       "        0.07334854, 0.05241536],\n",
       "       [0.05445921, 0.07529398, 0.06728566, 0.072414  , 0.05839157,\n",
       "        0.061341  , 0.07683461, 0.06966991, 0.06087468, 0.05962287,\n",
       "        0.05688172, 0.06250831, 0.06976258, 0.05960856, 0.06929576,\n",
       "        0.0986548 , 0.05716898, 0.07143921, 0.06263574, 0.07864494,\n",
       "        0.05833815, 0.06129941],\n",
       "       [0.07105702, 0.07491988, 0.05846037, 0.06097049, 0.0575617 ,\n",
       "        0.07147403, 0.05461457, 0.08017781, 0.08627757, 0.07666505,\n",
       "        0.05718416, 0.06425797, 0.06388654, 0.07382891, 0.05924578,\n",
       "        0.0623795 , 0.05843153, 0.08058508, 0.06679027, 0.05398253,\n",
       "        0.06232138, 0.07815383],\n",
       "       [0.05071252, 0.05677979, 0.05210699, 0.06021725, 0.07035456,\n",
       "        0.0635658 , 0.06714866, 0.06638366, 0.05976572, 0.07988414,\n",
       "        0.06103407, 0.06653156, 0.05830687, 0.0729091 , 0.07291653,\n",
       "        0.05818056, 0.08104706, 0.0723853 , 0.0797082 , 0.07183854,\n",
       "        0.07173385, 0.06263983],\n",
       "       [0.06436189, 0.07149482, 0.0710995 , 0.05858269, 0.05480545,\n",
       "        0.08831161, 0.08917477, 0.05568966, 0.07178405, 0.06305802,\n",
       "        0.0689209 , 0.05515607, 0.07317077, 0.07729606, 0.05372998,\n",
       "        0.05177025, 0.07821251, 0.07530827, 0.05213135, 0.07282059,\n",
       "        0.05654727, 0.0684436 ],\n",
       "       [0.05683437, 0.06981429, 0.06090588, 0.0539941 , 0.07437009,\n",
       "        0.07784089, 0.06574225, 0.06975928, 0.05861944, 0.07596132,\n",
       "        0.05865809, 0.05972828, 0.07516693, 0.07965807, 0.0727126 ,\n",
       "        0.06469397, 0.05753138, 0.054995  , 0.06720404, 0.07464661,\n",
       "        0.07149028, 0.06083564],\n",
       "       [0.05681027, 0.05833031, 0.07045493, 0.05571951, 0.06373923,\n",
       "        0.06082869, 0.05387949, 0.07355485, 0.08110701, 0.07232821,\n",
       "        0.07195358, 0.07592243, 0.06110214, 0.07098638, 0.06208161,\n",
       "        0.05693973, 0.07998197, 0.08038969, 0.06949776, 0.05505578,\n",
       "        0.07555758, 0.08680986]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4190, 0.6529],\n",
       "        [0.4190, 0.6529],\n",
       "        [0.4190, 0.6529],\n",
       "        ...,\n",
       "        [0.4190, 0.6529],\n",
       "        [0.4190, 0.6529],\n",
       "        [0.4190, 0.6529]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_logits[-2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
